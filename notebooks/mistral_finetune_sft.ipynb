{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral7B - SlimOrca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Accelerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import FullyShardedDataParallelPlugin,Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig,FullStateDictConfig \n",
    "\n",
    "fdsp_plugin  = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, ),\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(fsdp_plugin=fdsp_plugin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['conversations'],\n",
       "        num_rows: 363491\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Open-Orca/SlimOrca-Dedup\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prompt Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(example,add_generation=False):\n",
    "    template = ''\n",
    "    \n",
    "    for message in example['conversations']:\n",
    "        if add_generation and message['from'] == 'gpt':\n",
    "             continue\n",
    "        #Remove 'Answer:' from the start of the message\n",
    "        if message['from'] == 'human':\n",
    "            message['from'] = 'user'\n",
    "        if message['from'] == 'gpt':\n",
    "            message['from'] = 'assistant'\n",
    "        template += '<|im_start|>' + message['from'] + '\\n' + message['value'] + '<|im_end|>' + '\\n'\n",
    "        \n",
    "    \n",
    "    if add_generation:\n",
    "            template+='<|im_start|>assistant\\n'\n",
    "   \n",
    "\n",
    "    return {'text':template}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset: 290792\n",
      "test_dataset: 71972\n",
      "eval_dataset: 727\n"
     ]
    }
   ],
   "source": [
    "train_eval = dataset['train'].train_test_split(test_size=0.2, seed=42)\n",
    "test_eval = train_eval['test'].train_test_split(test_size=0.01, seed=42)\n",
    "\n",
    "train_dataset = train_eval['train'].map(formatting_func,remove_columns=['conversations'])\n",
    "test_dataset = test_eval['train'].map(formatting_func,remove_columns=['conversations'])\n",
    "eval_dataset = test_eval['test'].map(formatting_func,remove_columns=['conversations'])\n",
    "\n",
    "print(f'train_dataset: {len(train_dataset)}')\n",
    "print(f'test_dataset: {len(test_dataset)}')\n",
    "print(f'eval_dataset: {len(eval_dataset)}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"<|im_start|>system\\nYou are an AI assistant. Provide a detailed answer so user donâ€™t need to search outside to understand the answer.<|im_end|>\\n<|im_start|>user\\nAfter battling the way through traffic, Lee came to pick Quinn up from school.  Given the context: What will happen to Lee?\\nAnswer:<|im_end|>\\n<|im_start|>assistant\\nIn the given context, Lee has successfully navigated through traffic and has arrived at Quinn's school. What will happen to Lee next is that they will meet Quinn, presumably at a designated pickup area or outside of the school building. Once Quinn joins Lee, they will likely engage in conversation, perhaps discussing their respective days or commenting on the traffic situation. Afterward, Lee will drive from the school with Quinn, either heading back home or to another destination depending on their plans for the day.<|im_end|>\\n\"}\n",
      "{'text': '<|im_start|>system\\nYou are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.<|im_end|>\\n<|im_start|>user\\nPremise: a brown and white dog has is mouth open ready to catch a green ball.\\n\\nHypothesis: the bvoy was dressed as a cowboy\\n\\n.Can we conclude that the hypothesis is true if the premise is true?<|im_end|>\\n<|im_start|>assistant\\nAlright, let me explain it in a simple way. The premise is about a brown and white dog trying to catch a green ball. This information only talks about the dog and the ball.\\n\\nNow, the hypothesis is about a boy dressed as a cowboy. This information is talking about a boy and his costume.\\n\\nSince the premise only talks about the dog and the ball and does not mention anything about the boy or his costume, we cannot say that the hypothesis is true just because the premise is true. These two pieces of information are separate and not related to each other.<|im_end|>\\n'}\n",
      "{'text': \"<|im_start|>system\\nYou are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps.<|im_end|>\\n<|im_start|>user\\nCuriosity drove the archaeologist, more that the objects themselves he was always looking to what?  A. make money  B. go somewhere  C. design software  D. find truth  E. use television  The best answer is\\nAnswer:<|im_end|>\\n<|im_start|>assistant\\nD. find truth\\n\\nStep 1: Identify the subject of the sentence.\\nThe subject here is the archaeologist and their motivation.\\n\\nStep 2: Determine the context.\\nThe context is the archaeologist's driving force and their goals in pursuing their profession.\\n\\nStep 3: Evaluate each choice in relation to the context.\\nA. make money - This option may not primarily motivate an archaeologist and does not fit the context well.\\nB. go somewhere - This is vague and does not specifically relate to the archaeologist's profession or motivation.\\nC. design software - This is unrelated to archaeology and does not fit the context.\\nD. find truth - This option is consistent with the motivations of an archaeologist who seeks to uncover the past and discover information.\\nE. use television - This is unrelated to archaeology and does not fit the context.\\n\\nStep 4: Choose the best-fitting answer.\\nIn this case, discovering the truth aligns best with the motivations of an archaeologist.<|im_end|>\\n\"}\n"
     ]
    }
   ],
   "source": [
    "#Sanity check   \n",
    "print(train_dataset[0])\n",
    "print(test_dataset[0])\n",
    "print(eval_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Load the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d89d1b77445440749b9bf2abcb97eee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "import os,torch, wandb\n",
    "from trl import SFTTrainer\n",
    "import torch\n",
    "\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=False,\n",
    "            bnb_4bit_quant_type='nf4',\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             load_in_4bit=True,\n",
    "                                             quantization_config=bnb_config,\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             \n",
    "                                             device_map='auto',\n",
    "                                             trust_remote_code=True,\n",
    "                                             )\n",
    "\n",
    "model.config.use_cache=False \n",
    "model.config.pretraining_tp = 1\n",
    "model.gradient_checkpointing_enable()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Load the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.add_bos_token, tokenizer.add_eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Set up LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 0.0M || all params: 3752.071168M || trainable%: 0.0\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params/1e6}M || all params: {all_param/1e6}M || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 170.082304M || all params: 3922.153472M || trainable%: 4.336452033664837\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",],\n",
    "    bias='none',\n",
    "    lora_dropout=0.1,\n",
    "    task_type='CAUSAL_LM'\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if torch.cuda.device_count() > 1:\n",
    "#     print('Using DataParallel')\n",
    "#     print('Device:', [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())])\n",
    "#     model.is_parallelizable = True\n",
    "#     model.model_parallel = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk.translate.bleu_score as bleu\n",
    " \n",
    " \n",
    "def calculate_BLEU(generated_summary, reference_summary, n=2):\n",
    "    # Tokenize the generated summary and reference summary\n",
    "    generated_tokens = generated_summary.split()\n",
    "    reference_tokens = reference_summary.split()\n",
    " \n",
    "    # Calculate the BLEU score\n",
    "    weights = [1.0 / n] * n  # Weights for n-gram precision calculation\n",
    "    bleu_score = bleu.sentence_bleu([reference_tokens], generated_tokens, weights=weights)\n",
    " \n",
    "    return bleu_score\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def generate_ngrams(text, n):\n",
    "    # Preprocess text by removing punctuation and converting to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    " \n",
    "    # Generate n-grams from the preprocessed text\n",
    "    words = text.split()\n",
    "    ngrams = [tuple(words[i:i+n]) for i in range(len(words) - n + 1)]\n",
    " \n",
    "    return ngrams\n",
    "\n",
    "def calculate_ROUGE(generated_summary, reference_summary, n=2):\n",
    "    # Tokenize the generated summary and reference summary into n-grams\n",
    "    generated_ngrams = generate_ngrams(generated_summary, n)\n",
    "    reference_ngrams = generate_ngrams(reference_summary, n)\n",
    " \n",
    "    # Calculate the recall score\n",
    "    matching_ngrams = len(set(generated_ngrams) & set(reference_ngrams))\n",
    "    recall_score = matching_ngrams / len(reference_ngrams)\n",
    " \n",
    "    return recall_score\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Calculate BLEU score\n",
    "    bleu_scores = []\n",
    "    for i in range(len(predictions)):\n",
    "        bleu_score = calculate_BLEU(predictions[i], labels[i])\n",
    "        bleu_scores.append(bleu_score)\n",
    "    avg_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
    "\n",
    "    # Calculate ROUGE score\n",
    "    rouge_scores = []\n",
    "    for i in range(len(predictions)):\n",
    "        rouge_score = calculate_ROUGE(predictions[i], labels[i])\n",
    "        rouge_scores.append(rouge_score)\n",
    "    avg_rouge_score = sum(rouge_scores) / len(rouge_scores)\n",
    "\n",
    "    return { \"bleu\": avg_bleu_score, \"rouge\": avg_rouge_score }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_dir: ./Mistral7B-SlimOrca-PEFT-2024-02-05-22-04\n",
      "run_name: Mistral7B-SlimOrca-PEFT-2024-02-05-22-04\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "\n",
    "dataset ='SlimOrca'\n",
    "base_model_name='Mistral7B'\n",
    "project='PEFT'\n",
    "run_name = f'{base_model_name}-{dataset}-{project}-{datetime.now().strftime(\"%Y-%m-%d-%H-%M\")}'\n",
    "output_dir = f'./{run_name}'\n",
    "\n",
    "print(f'output_dir: {output_dir}')\n",
    "print(f'run_name: {run_name}')\n",
    "\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=25,\n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=10,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    # report_to=\"wandb\",\n",
    "    report_to=\"none\",\n",
    "    run_name=run_name,\n",
    "    do_eval=True,\n",
    "    eval_steps=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length= 512,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    # formatting_func=formatting_func,\n",
    "    packing= False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2/10 : < :, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='52' max='91' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [52/91 05:23 < 04:07, 0.16 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
